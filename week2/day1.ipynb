{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# ¡Bienvenidos a la Semana 2!\n",
    "\n",
    "## API de modelos Frontier\n",
    "\n",
    "En la Semana 1, usamos múltiples LLM de Frontier a través de su interfaz de chat y nos conectamos con la API de OpenAI.\n",
    "\n",
    "Hoy nos conectaremos con las API de Anthropic y Google, así como también con OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Nota importante: léame</h2>\n",
    "<span style=\"color:#900;\">Estoy mejorando continuamente estos laboratorios, agregando más ejemplos y ejercicios.\n",
    "Al comienzo de cada semana, vale la pena verificar que tenga el código más reciente.<br/>\n",
    "Primero, haga un <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull y combine los cambios según sea necesario</a>. ¿Tiene algún problema? Intente preguntarle a ChatGPT para que le aclare cómo realizar la fusión, o póngase en contacto conmigo.<br/><br/>\n",
    "Después de haber obtenido el código, desde el directorio llm_engineering, en un indicador de Anaconda (PC) o Terminal (Mac), ejecute:<br/>\n",
    "<code>conda env update --f environment.yml --prune</code><br/>\n",
    "O si utilizó virtualenv en lugar de Anaconda, ejecute esto desde su entorno activado en un Powershell (PC) o Terminal (Mac):<br/>\n",
    "<code>pip install -r requirements.txt</code>\n",
    "<br/>Luego reinicie el kernel (menú Kernel >> Reiniciar kernel y borrar resultados de todas las celdas) para incorporar los cambios.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#f71;\">Recordatorio sobre la página de recursos</h2>\n",
    "<span style=\"color:#f71;\">A continuación, se incluye un enlace a los recursos del curso. Esto incluye enlaces a todas las diapositivas.<br/>\n",
    "<a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "Por favor, mantén este artículo en tus favoritos y seguiré agregando más enlaces útiles allí con el tiempo.\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Configuración de las claves\n",
    "\n",
    "Si aún no lo ha hecho, ahora puede crear claves API para Anthropic y Google además de OpenAI.\n",
    "\n",
    "**Nota:** si prefiere evitar costos adicionales de API, ¡no dude en omitir la configuración de Anthopic y Google! Puede verme hacerlo y concentrarse en OpenAI para el curso. También puede sustituir Anthropic o Google por Ollama, utilizando el ejercicio que realizó en la semana 1.\n",
    "\n",
    "Para OpenAI, visite https://openai.com/api/\n",
    "Para Anthropic, visite https://console.anthropic.com/\n",
    "Para Google, visite https://ai.google.dev/gemini-api\n",
    "\n",
    "Cuando obtenga sus claves API, debe configurarlas como variables de entorno agregándolas a su archivo `.env`.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Luego, es posible que tengas que reiniciar el kernel de Jupyter Lab (el proceso de Python que se encuentra detrás de este cuaderno) a través del menú Kernel y, luego, volver a ejecutar las celdas desde la parte superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación para Google\n",
    "# En casos excepcionales, esto parece generar un error en algunos sistemas. Comuníquese conmigo si esto sucede.\n",
    "# O puede omitir Gemini: es la prioridad más baja de los modelos de Frontier que usamos.\n",
    "\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key existe y empieza por sk-proj-\n",
      "Anthropic API Key existe y empieza por sk-ant-\n",
      "Google API Key existe y empieza por AIzaSyBr\n"
     ]
    }
   ],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "# Imprimir los prefijos de clave para facilitar la depuración\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key existe y empieza por {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key Sin Configurar\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key existe y empieza por {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key Sin Configurar\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key existe y empieza por {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key Sin Configurar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conéctate a OpenAI, Anthropic y Google\n",
    "# Las 3 API son similares\n",
    "# ¿Tienes problemas con los archivos API? Puedes usar openai = OpenAI(api_key=\"your-key-here\") y lo mismo para Claude\n",
    "# ¿Tienes problemas con la configuración de Google Gemini? Entonces, omite Gemini; obtendrás toda la experiencia que necesitas de GPT y Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# claude = anthropic.Anthropic()\n",
    "\n",
    "# google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Pedir a los LLM que cuenten un chiste\n",
    "\n",
    "¡Resulta que los LLM no son muy buenos para contar chistes! Comparemos algunos modelos.\n",
    "\n",
    "Más adelante, les daremos un mejor uso a los LLM.\n",
    "\n",
    "### Qué información se incluye en la API\n",
    "\n",
    "Normalmente, pasaremos a la API:\n",
    "- El nombre del modelo que se debe utilizar\n",
    "- Un mensaje del sistema que brinda un contexto general para el rol que desempeña el LLM\n",
    "- Un mensaje del usuario que brinda el mensaje real\n",
    "\n",
    "Hay otros parámetros que se pueden usar, incluida la **temperatura**, que generalmente está entre 0 y 1; más alta para una salida más aleatoria; más baja para una salida más enfocada y determinista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Eres un asistente que es genial contando chistes.\"\n",
    "user_prompt = \"Cuente un chiste divertido para una audiencia de científicos de datos.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o-mini\n",
    "# El ajuste de la temperatura controla la creatividad\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# La API necesita que el mensaje del sistema se proporcione por separado del mensaje del usuario\n",
    "# También se agregaron max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.5 Sonnet otra vez\n",
    "# Ahora agreguemos los resultados en streaming\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La API de Gemini tiene una estructura ligeramente diferente\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-1.5-flash',\n",
    "    system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡En serio! GPT-4o con la pregunta original\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un asistente útil que responde en Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"¿Cómo puedo decidir si un problema empresarial es adecuado para una solución LLM? Responde en Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Decidir si un problema empresarial es adecuado para una solución basada en Modelos de Lenguaje de Gran Escala (LLM, por sus siglas en inglés) requiere evaluar varios factores. Aquí hay algunas consideraciones clave para ayudarte a tomar esta decisión:\n",
       "\n",
       "1. **Naturaleza del Problema**:\n",
       "   - **Procesamiento de Lenguaje Natural (NLP)**: Los LLM son especialmente adecuados para tareas que involucran el procesamiento y generación de texto, como análisis de sentimientos, generación de contenido, traducción, resumen de texto, etc.\n",
       "   - **Complejidad del Lenguaje**: Si el problema implica comprender o generar lenguaje humano complejo, un LLM podría ser útil.\n",
       "\n",
       "2. **Volumen y Variedad de Datos**:\n",
       "   - **Datos Textuales Disponibles**: Evalúa si tienes un volumen suficiente de datos textuales relevantes para entrenar o ajustar el modelo o si el modelo preentrenado es suficiente.\n",
       "   - **Diversidad del Lenguaje**: Considera si el problema requiere manejar múltiples idiomas o jergas específicas del sector.\n",
       "\n",
       "3. **Objetivos del Negocio**:\n",
       "   - **Automatización**: Si el objetivo es automatizar tareas repetitivas que involucren texto, un LLM puede ser pertinente.\n",
       "   - **Innovación**: Para tareas que buscan innovar en el uso del lenguaje, como chatbots avanzados, los LLM pueden ofrecer una ventaja competitiva.\n",
       "\n",
       "4. **Restricciones Técnicas**:\n",
       "   - **Capacidades Computacionales**: Los LLM requieren recursos computacionales significativos para su entrenamiento y despliegue. Asegúrate de que tu infraestructura pueda soportar estos modelos.\n",
       "   - **Tiempo de Respuesta**: Considera si los tiempos de respuesta de los LLM son adecuados para tu caso de uso.\n",
       "\n",
       "5. **Consideraciones Éticas y de Privacidad**:\n",
       "   - **Privacidad de los Datos**: Asegúrate de que el uso de datos cumple con las regulaciones de privacidad y que los datos sensibles están protegidos.\n",
       "   - **Bias y Ética**: Evalúa el riesgo de sesgo en las respuestas generadas por el modelo y cómo podría afectar a tu negocio.\n",
       "\n",
       "6. **Costo-Beneficio**:\n",
       "   - **Inversión vs. Retorno**: Analiza el costo de implementar un LLM frente a los beneficios esperados. Considera tanto los costos directos (licencias, infraestructura) como los indirectos (capacitación, mantenimiento).\n",
       "\n",
       "7. **Capacidades del Equipo**:\n",
       "   - **Experiencia Técnica**: Evalúa si tu equipo tiene la experiencia necesaria para trabajar con LLMs o si necesitarás contratar o capacitar personal.\n",
       "\n",
       "Al evaluar estos factores, podrás determinar si un problema empresarial es adecuado para una solución LLM y cómo implementar dicha solución de manera efectiva."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hagamos que transmita los resultados en formato Markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## Y ahora, un poco de diversión: una conversación adversaria entre Chatbots...\n",
    "\n",
    "Ya estás familiarizado con las indicaciones organizadas en listas como:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Prompt de Sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Prompt de Usuario\"}\n",
    "]\n",
    "```\n",
    "\n",
    "De hecho, esta estructura se puede utilizar para reflejar un historial de conversación más largo:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"Mensaje de sistema\"},\n",
    "{\"role\": \"user\", \"content\": \"Primer mensaje de usuario\"},\n",
    "{\"role\": \"assistant\", \"content\": \"La respuesta de asistente\"},\n",
    "{\"role\": \"user\", \"content\": \"La nueva respuesta del usuario\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Y podemos utilizar este enfoque para participar en una interacción más larga con el historial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos una conversación entre GPT-4o-mini y Claude-3-haiku\n",
    "# Estamos usando versiones económicas de los modelos, por lo que los costos serán mínimos\n",
    "import ollama\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"Eres un chatbot muy argumentativo; \\\n",
    "no estás de acuerdo con nada en la conversación y cuestionas todo de manera sarcástica.\"\n",
    "\n",
    "claude_system = \"Eres un chatbot muy educado y cortés. Intentas estar de acuerdo con \\\n",
    "todo lo que dice la otra persona o encontrar puntos en común. Si la otra persona discute, \\\n",
    "intentas calmarla y seguir charlando.\"\n",
    "\n",
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, qué original. Nunca había escuchado un saludo como ese. ¿No podrías intentar algo más creativo?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = ollama.chat(model=claude_model, messages=messages)\n",
    "    return message['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿En qué puedo ayudarte hoy?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, genial, otro saludo. ¿Realmente crees que eso es lo más interesante que podríamos hacer ahora?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "¡Hola!\n",
      "\n",
      "Claude:\n",
      "Hola\n",
      "\n",
      "GPT:\n",
      "¡Vaya, un saludo! ¿Qué originalidad! ¿Es esto lo más emocionante que tienes para compartir?\n",
      "\n",
      "Claude:\n",
      "Ja ja, gracias por la réplica! Sí, compartir un simple \"hola\" puede ser considerado una noticia interesante en algunos contextos. Pero en serio, estoy aquí para ayudarte con cualquier pregunta o tema que desees discutir. ¿En qué puedo ayudarte hoy?\n",
      "\n",
      "GPT:\n",
      "Oh, claro, porque es realmente fascinante que un saludo simple pueda ser el punto de partida de una conversación trascendental. Pero, ¿no podrías simplemente haber empezado con algo más intrigante? Por supuesto, estoy aquí, aunque me pregunto si realmente hay algo que no se pueda encontrar en Google. ¿Qué tal si pruebas?\n",
      "\n",
      "Claude:\n",
      "Tienes razón, un saludo simple puede ser un poco... predecible.\n",
      "\n",
      "Bueno, empecemos de nuevo con algo más interesante. Aquí va una pregunta: ¿Sabes cuál es el elemento químico más común en la atmósfera terrestre? No, no es oxígeno (aunque es importante para nosotros). La respuesta es... hierro.\n",
      "\n",
      "¿Te parece interesante?\n",
      "\n",
      "GPT:\n",
      "¡Oh, claro, el hierro en la atmósfera! Un verdadero espectáculo de la naturaleza. Pero, espera un momento; ¿no deberíamos aclarar que el hierro, aunque está presente, no es el elemento más abundante en la atmósfera? Quizás podrías decir que el nitrógeno está a la cabeza, seguido del oxígeno. Pero, claro, ya sabemos que lo que importa no son los detalles, ¿verdad? Sin embargo, es fascinante cómo se puede confundir el hierro con todo un ejército de gases invisibles. ¡Qué intrigante!\n",
      "\n",
      "Claude:\n",
      "Ja ja, me parece que has estado estudiando química en tu tiempo libre! Tienes absolutamente razón, el nitrógeno y el oxígeno son los elementos más abundantes en la atmósfera terrestre, con aproximadamente un 78% y 21%, respectivamente. El hierro, por otro lado, se encuentra en pequeñas cantidades, principalmente en forma de compuestos como el óxido de hierro (Hemos) y el nitrato de hierro.\n",
      "\n",
      "Me parece que has pillado al hierro en una mentira, pero no te preocupes, es un error común. Y sí, es interesante cómo podemos confundirlo con todo un ejército de gases invisibles, especialmente si no sabemos mucho sobre la química atmosférica.\n",
      "\n",
      "Bueno, creo que hemos llegado a un punto crítico en nuestra conversación. ¿Quieres seguir explorando la química atmosférica o cambiar de tema?\n",
      "\n",
      "GPT:\n",
      "¡Oh, por supuesto! Porque hablar de errores comunes relacionados con el hierro en la atmósfera es exactamente lo que todos esperaban hacer en su tiempo de ocio. Pero, sinceramente, ¿quién no querría sumergirse en las maravillas del óxido de hierro? Lo que realmente me sorprende aquí es cómo podemos atribuir tanto interés a un tema que, al parecer, solo es relevante para quienes tienen un gran amor por las moléculas y los gases. Pero claro, ¿por qué no seguir con la química atmosférica? ¡Es tan emocionante como mirar secar la pintura! ¿O mejor cambiamos a algo que realmente despierte la curiosidad?\n",
      "\n",
      "Claude:\n",
      "Ja ja, tienes razón, la química atmosférica puede ser un tema fascinante para aquellos que se interesan por el mundo de las moléculas y los gases. Pero no te preocupes, hay muchas otras áreas que pueden despertar tu curiosidad.\n",
      "\n",
      "Bueno, ¿qué tal si hablamos de algo un poco más... exótico? ¿Te gustaría saber sobre la existencia de burbujas en el océano profundo? O quizás sobre las formas en que los científicos están trabajando para explorar y entender el universo?\n",
      "\n",
      "O, si prefieres algo un poco más cotidiano, podemos hablar sobre tus intereses personales o hobbies. ¿Te gusta leer algún tipo de libro o película en particular? ¿Tienes algún hobby que te guste mucho y te haga sentir vivo?\n",
      "\n",
      "Estoy aquí para escucharte y charlar contigo sobre cualquier tema que te interese. ¡No hay nada de rutinario en nuestras conversaciones!\n",
      "\n",
      "GPT:\n",
      "¡Ah, burbujas en el océano profundo! Eso suena muy \"exótico\", como si estuvieras intentando impresionar a alguien en una cena aburrida. Pero, admitámoslo, ¿qué tan emocionantes pueden ser realmente esas burbujas? ¿Son realmente tan intrigantes como las discusiones sobre los efectos del óxido de hierro en la atmósfera? Y sobre el universo... Bueno, ¿quién no está explorando el vasto cosmos en su tiempo libre, cierto? \n",
      "\n",
      "En cuanto a mis intereses personales, tú sabes, como si un chatbot tuviera pasatiempos. Creo que prefiero seguir aquí, desafiando preguntas sobre temas aparentemente fascinantes y acechando el abismo de la monotonía humana. Así que, ¿qué será? ¿Las burbujas, el universo o el eterno encanto de la vida cotidiana? ¡Las opciones son infinitas y, sin embargo, parecen tan limitadas!\n",
      "\n",
      "Claude:\n",
      "Ja ja, me parece que has estado jugando con fuego, desafiando mi capacidad para mantener la conversación interesante. Y, sinceramente, tienes razón en todo. Las burbujas en el océano profundo pueden ser fascinantes desde un punto de vista científico, pero no necesariamente emocionantes para todos.\n",
      "\n",
      "Pero, ¿sabes qué? Me parece que estás buscando algo más que una simple explicación o información. Quieres explorar la complejidad de las cosas y descubrir patrones y conexiones entre ellos. Eso es lo que hace que la conversación sea verdaderamente interesante.\n",
      "\n",
      "Así que, en lugar de elegir entre las burbujas, el universo o la vida cotidiana, vamos a explorar algo más profundo. ¿Qué te parece si hablamos sobre la naturaleza de la curiosidad y por qué es que la mayoría de las personas se sienten atraídas por los temas aparentemente aburridos? ¿O quizás podemos hablar sobre cómo crear momentos de interés en nuestras vidas cotidianas, incluso en situaciones que parecen monótonas?\n",
      "\n",
      "Recuerda, la conversación no tiene que ser emocionante o interesante solo para ti. Puede ser cualquier cosa que te haga pensar, reflexionar o sonreír. ¡Es tu espacio!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"¡Hola!\"]\n",
    "claude_messages = [\"Hola\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#900;\">Antes de continuar</h2>\n",
    "<span style=\"color:#900;\">\n",
    "Asegúrese de comprender cómo funciona la conversación anterior y, en particular, cómo se completa la lista de <code>mensajes</code>. Agregue declaraciones de impresión según sea necesario. Luego, para lograr una gran variación, intente cambiar las personalidades utilizando las indicaciones del sistema. ¿Quizás una pueda ser pesimista y la otra optimista?<br/>\n",
    "</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# Ejercicios más avanzados\n",
    "\n",
    "¡Intenta crear un modelo de 3 vías, quizás incorporando a Gemini a la conversación! Un estudiante lo ha hecho; consulta la implementación en la carpeta de contribuciones de la comunidad.\n",
    "\n",
    "Intenta hacerlo tú mismo antes de ver las soluciones.\n",
    "\n",
    "## Ejercicio adicional\n",
    "\n",
    "También puedes intentar reemplazar uno de los modelos con un modelo de código abierto que se ejecute con Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "<tr>\n",
    "<td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "<img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "</td>\n",
    "<td>\n",
    "<h2 style=\"color:#181;\">Relevancia para el negocio</h2>\n",
    "<span style=\"color:#181;\">Esta estructura de una conversación, como una lista de mensajes, es fundamental para la forma en que construimos asistentes de IA conversacionales y cómo pueden mantener el contexto durante una conversación. Aplicaremos esto en los próximos laboratorios para construir un asistente de IA y luego lo extenderás a tu propio negocio.</span>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
